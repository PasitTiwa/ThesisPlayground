{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.clip as clip\n",
    "from model.model import ResidualAttentionBlock\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in skimage to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "  \n",
    "    plt.subplot(2, 4, len(images) + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(descriptions[name])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_image_similarity(image_feature, text_feature):\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    similarity_patches = []\n",
    "    for feature_vector in image_feature[1:50]:\n",
    "        similarity = cos(text_feature[0], feature_vector).cpu()\n",
    "        similarity_patches.append(similarity)\n",
    "    cls_similarity = cos(image_feature[0], text_feature[0]).cpu()\n",
    "    return similarity_patches, cls_similarity\n",
    "\n",
    "def build_similarity_map(similarity_patches, image_size, patch_size):\n",
    "    num_patch_per_row = image_size / patch_size\n",
    "    temp_row = []\n",
    "\n",
    "    for index, patches in enumerate(similarity_patches):\n",
    "        if index % num_patch_per_row == 0:\n",
    "            if index != 0:\n",
    "                temp_row.append(temp_column)\n",
    "                temp_column = torch.full((patch_size, patch_size), patches)\n",
    "            else:\n",
    "                temp_column = torch.full((patch_size, patch_size), patches)\n",
    "        else:\n",
    "            temp_filter = torch.full((patch_size, patch_size), patches)\n",
    "            temp_column = torch.cat((temp_column, temp_filter), -1)\n",
    "    temp_row.append(temp_column)\n",
    "    image_filter = torch.cat(temp_row, 0)\n",
    "    return image_filter\n",
    "\n",
    "def plot_image_similarity(image, image_filter, text):\n",
    "    plt.imshow(image.permute(1,2,0), interpolation='nearest')\n",
    "    plt.title(text)\n",
    "    plt.imshow(image_filter, alpha=0.4, cmap='jet')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_no = 4\n",
    "text_no = 4\n",
    "patch_size = 32\n",
    "image_size = 224\n",
    "\n",
    "# text_feature_test = text_feature_test\n",
    "# text_test = \"This is a image of stand camera\"\n",
    "text_test = texts[text_no]\n",
    "text_feature_test = text_features[text_no]\n",
    "image_feature_test = image_features[image_no]\n",
    "image_test = images[image_no]\n",
    "\n",
    "print(\"Image Discription:\", text_test)\n",
    "\n",
    "similarity_per_patch, cls_similarity = text_image_similarity(image_feature_test, text_feature_test)\n",
    "print(\"Cls image similarity:\", cls_similarity)\n",
    "\n",
    "image_similarity_map = build_similarity_map(similarity_per_patch, image_size,  patch_size)\n",
    "plot_image_similarity(image_test, image_similarity_map, text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_per_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
